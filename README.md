Vera API ğŸ¤–

Vera is a personal-use AI assistant, designed for local experimentation and future evolution into a potential SaaS product.

It features:

Long-term conversation memory (vector store)

Retrieval-Augmented Generation (RAG)

Streaming LLM responses via Ollama

Optional speech-to-text and text-to-speech modules

Secure public access via Cloudflare Tunnel (free)

This repository exposes Vera as an HTTP API, allowing usage from:

Postman

Web applications (Vue / React)

Mobile applications (iOS / Android)

Any internet-connected client

âœ¨ Features

ğŸ§  Conversation Memory â€“ semantic recall of relevant past interactions

ğŸ” RAG Pipeline â€“ vector search + external context (e.g. Twitch chat)

ğŸ’¬ Chat API â€“ simple and extensible /chat endpoint

ğŸ–¥ï¸ CLI Interface â€“ legacy local assistant for fast iteration

ğŸ—£ï¸ Speech-to-Text â€“ Whisper (default), Vosk (offline fallback)

ğŸ”Š Text-to-Speech â€“ XTTS (disabled by default due to latency)

ğŸŒ Internet Exposure â€“ free HTTPS access via Cloudflare Tunnel

ğŸ” Optional Authentication â€“ API keyâ€“based security

ğŸ—ï¸ Project Structure
.
â”œâ”€â”€ api.py                     # FastAPI entrypoint (HTTP API)
â”œâ”€â”€ vera_core.py               # Core Vera engine (LLM + memory + RAG)
â”œâ”€â”€ vera_cli.py                # Legacy CLI interface for local testing
â”œâ”€â”€ authorization.py           # Optional API key authentication
â”œâ”€â”€ db.py                      # Conversation persistence
â”œâ”€â”€ vector_store.py            # Embeddings + vector database logic
â”œâ”€â”€ query_builder.py           # Query expansion logic
â”œâ”€â”€ external_rag_module.py     # External RAG sources (e.g. Twitch)
â”œâ”€â”€ speech_to_text_whisper.py  # Whisper STT (default)
â”œâ”€â”€ speech_to_text_vosk.py     # Vosk STT (offline fallback)
â”œâ”€â”€ text_to_speech_xtts.py     # XTTS TTS (optional)
â”œâ”€â”€ schema.sql                 # Database schema (table creation)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Requirements

Python 3.10+

Ollama installed and running

Supported LLM model (e.g. llama3)

ğŸ“¦ Installation
pip install -r requirements.txt


Ensure Ollama is running:

ollama serve


Pull the model:

ollama pull llama3

â–¶ï¸ Run the API Locally
uvicorn api:app --host 0.0.0.0 --port 8000


API available at:

http://localhost:8000


Swagger UI:

http://localhost:8000/docs

ğŸ§ª Test with Postman
Endpoint

POST /chat

Request Body
{
  "message": "Hello Vera"
}

Response
{
  "session_id": "uuid",
  "response": "Hello! How can I help you today?"
}

ğŸ–¥ï¸ Run Vera via CLI (Legacy)

Before the API existed, Vera was used as a local CLI assistant for rapid testing.

python vera_cli.py


This mode supports:

Local conversation loop

Memory recall

Optional voice input/output

âš ï¸ The CLI is considered legacy and intended mainly for development/debugging.

ğŸŒ Expose API to the Internet (FREE)
cloudflared tunnel --url http://localhost:8000


Example public URL:

https://random-name.trycloudflare.com


No domain or payment required.

ğŸ” Security

Authentication is optional by design.

Example header:

Authorization: Bearer YOUR_API_KEY

ğŸ”Š Voice (Optional)
Speech-to-Text

Whisper (default)

Vosk (offline fallback)

Text-to-Speech

XTTS (disabled by default)

Outputs .wav files

ğŸ§  Architecture
Client (API / CLI)
        |
        v
     FastAPI
        |
        v
   VeraEngine
        |
        +--> Memory (Vector Store)
        |
        +--> External RAG
        |
        v
      LLM (Ollama)

ğŸ—„ï¸ Database

Schema defined in schema.sql.

â¤ï¸ Credits

Built by Hoang Nguyen The